import torch
import torch.nn as nn
import snntorch as snn
from snntorch import surrogate
from backbone import MammothBackbone, register_backbone


def conv3x3(in_channels: int, out_channels: int, stride: int = 1) -> nn.Conv2d:
    """Pre-configured 2D 3x3 convolution to use in SResNet architectures.

    Args:
        in_channels: Number of input channels C_in.
        out_channels: Number of kernels C_out.
        stride: Stride to apply over the spatial dimensionality.

    Returns:
        nn.Conv2d: A pre-configured 2D 3x3 convolution.
    """
    return nn.Conv2d(
        in_channels=in_channels,
        out_channels=out_channels,
        kernel_size=3,
        stride=stride,
        padding=1,
        bias=False
    )


def conv1x1(in_channels: int, out_channels: int, stride: int = 1) -> nn.Conv2d:
    """Pre-configured 2D 1x1 convolution to use in SResNet architectures.

    Args:
        in_channels: Number of input channels C_in.
        out_channels: Number of kernels C_out.
        stride: Stride to apply over the spatial dimensionality.

    Returns:
        nn.Conv2d: A pre-configured 2D 1x1 convolution.
    """
    return nn.Conv2d(
        in_channels=in_channels,
        out_channels=out_channels,
        kernel_size=1,
        stride=stride,
        bias=False
    )


class LayerTWrapper(nn.Module):
    """Wraps a spatial layer to apply it independently along the temporal dimension.

    The batch normalization doesn't work for linear layers.

    Attributes:
        layer: The spatial layer to execute at each time step.
        batch_norm: Batch normalization layer that expects input of shape [B, C, T, H, W].
    """

    def __init__(self, layer: nn.Module, batch_norm: nn.Module = None) -> None:
        """Initializes the LayerTWrapper.

        Args:
            layer: A PyTorch layer that processes a single time slice of shape [B, C, H, W] or [B, N].
            batch_norm: A batch normalization layer that can accept a tensor of shape [T, B, C, H, W]
                and perform normalization across time and spatial dimensions.
        """
        super(LayerTWrapper, self).__init__()

        # Layers attributes
        self.layer = layer
        self.batch_norm = batch_norm

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies the wrapped layer across the time dimension and then optional batch normalization.

        This approach vectorizes the time dimension and allow the layer to process all the time steps in one pass.

        Args:
            x: Input tensor of shape [T, B, C, H, W] or [T, B, N].

        Returns:
            torch.Tensor: Output tensor of shape [T, B, C, H, W] or [T, B, N] normalized along
                the temporal and spatial dimensions.
        """
        # Retrieves the time steps and batch-size of the input tensor
        # 'in_spatial' could be [C, H, W] or [N]
        T, B, *in_spatial = x.shape

        # Collapses the time and batch dimensions
        x = x.reshape(T * B, *in_spatial)

        # One forward pass for the layer
        x = self.layer(x)

        # Retrieves the new spatial dimension
        _, *out_spatial = x.shape
        x = x.reshape(T, B, *out_spatial)

        # Applies batch normalization if present
        if self.batch_norm is not None:
            # Changes the order of dimensions in the tensor
            x = x.permute(1, 2, 0, 3, 4).contiguous()

            # Applies batch normalization
            x = self.batch_norm(x)

            # Changes the order of dimensions in the tensor
            x = x.permute(2, 0, 1, 3, 4).contiguous()

        return x


class LIFTWrapper(nn.Module):
    """Layer that wraps snnTorch Leaky layer to allow it to integrate over input spikes.

    Attributes:
        layer: An snnTorch Leaky neuron layer that performs membrane potential integration
            and spike generation per time step.
    """

    def __init__(self, layer: snn.Leaky) -> None:
        """Initializes the LIFTWrapper.

        Args:
            layer: A preconfigured snnTorch Leaky neuron layer.
        """
        super(LIFTWrapper, self).__init__()

        self.layer = layer

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies the Leaky layer over the time dimension of the input spike train.

        Iterates through T time steps, feeding each slice x[t] of shape [B, C, H, W] into the Leaky
        layer. Collects the output spikes at each step, stacks them into a tensor of shape
        [T, B, C, H, W], and then resets the Leaky layer's hidden and membrane states.

        Args:
            x: Input spike train tensor of shape [T, B, C, H, W].

        Returns:
            torch.Tensor: Output spike tensor of shape [T, B, C, H, W], containing the spikes
                generated by the Leaky layer at each time step.
        """
        # List of outputs spikes
        x_out = []

        # Retrieves the number of time steps, x.shape: [T, B, C, H, W]
        T = x.size(0)

        # Forward pass in time
        for t in range(T):
            # Retrieves the sample at time step t, x_t.shape: [B, C, H, W]
            x_t = x[t]

            # Appends the layer output
            x_out.append(self.layer(x_t))

        # Converts the list into a tensor
        x_out = torch.stack(x_out)

        # Resets the membrane potential and the hidden state to avoid backprop errors
        self.layer.reset_hidden()
        self.layer.reset_mem()

        return x_out


class SResNetBlock(nn.Module):
    """SResNet basic block.

    Spiking version of the classic ResNet basic block.

    Attributes:
        t_conv_bn1: First 3x3conv followed by a batch normalization.
        lif1: LIF neuron after the first convolution.
        t_conv_bn2: Second 3x3conv followed by a batch normalization.
        lif2: LIF neuron after second convolution.
        shortcuts: Identity mapping if channels don't change, 1x1 projection otherwise.
        lif3: Final LIF neuron applied after summing the residual and shortcut paths.
    """

    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            stride: int,
            beta: float,
            threshold: float,
            spike_grad,
            learn_beta: bool = False,
            learn_threshold: bool = False
    ) -> None:
        """Initializes the SResNetBlock.

        Args:
            in_channels: Number of input channels.
            out_channels: Number of output channels.
            stride: Stride to apply in the first 3x3conv and in the 1x1 projection if needed.
            beta: Membrane decay parameter.
        """
        super(SResNetBlock, self).__init__()

        # Main branch
        self.t_conv_bn1 = LayerTWrapper(
            layer=conv3x3(in_channels=in_channels, out_channels=out_channels, stride=stride),
            batch_norm=nn.BatchNorm3d(num_features=out_channels)
        )
        self.lif1 = LIFTWrapper(layer=snn.Leaky(
            beta=beta,
            threshold=threshold,
            init_hidden=True,
            learn_beta=learn_beta,
            learn_threshold=learn_threshold,
            spike_grad=spike_grad
        ))

        self.t_conv_bn2 = LayerTWrapper(
            layer=conv3x3(in_channels=out_channels, out_channels=out_channels),
            batch_norm=nn.BatchNorm3d(num_features=out_channels)
        )
        self.lif2 = LIFTWrapper(layer=snn.Leaky(
            beta=beta,
            threshold=threshold,
            init_hidden=True,
            learn_beta=learn_beta,
            learn_threshold=learn_threshold,
            spike_grad=spike_grad
        ))

        # Shortcut branch
        self.shortcuts = None
        if stride != 1 or in_channels != out_channels:
            self.shortcuts = LayerTWrapper(
                layer=conv1x1(in_channels=in_channels, out_channels=out_channels, stride=stride),
                batch_norm=nn.BatchNorm3d(num_features=out_channels)
            )

        self.lif3 = LIFTWrapper(layer=snn.Leaky(
            beta=beta,
            threshold=threshold,
            init_hidden=True,
            learn_beta=learn_beta,
            learn_threshold=learn_threshold,
            spike_grad=spike_grad
        ))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass for the basic block.

        Args:
            x: Input tensor of shape [T, B, C, H, W].

        Returns:
            torch.Tensor: The output of the basic block of shape [T, B, C, H, W].
        """
        identity = x

        # Main branch
        x = self.t_conv_bn1(x)
        x = self.lif1(x)

        x = self.t_conv_bn2(x)
        x = self.lif2(x)

        # Shortcut branch
        if self.shortcuts is not None:
            identity = self.shortcuts(identity)

        # Add and final LIF
        x += identity

        return self.lif3(x)


class SResNet(MammothBackbone):
    """Spiking ResNet generic architecture.

    This spiking version of ResNet can be customized to create different size of the architecture.

    Attributes:
        stem: 3Ã—3 convolution followed by temporal batch normalization.
        lif_stem: LIF neuron layer after the stem.
        stages: List containing the residual stages of the model (its size depends on the architecture).
        t_avg_pool: Global average-pooling layer applied over time.
        mlp: Classification head that maps the features to 'num_classes' logits per time step.
    """

    def __init__(
            self,
            in_channels: int,
            num_classes: int,
            beta: float,
            threshold: float,
            spike_grad,
            stem_channels: int,
            stage_blocks: list[int],
            stage_channels: list[int],
            learn_beta: bool = False,
            learn_threshold: bool = False,
    ) -> None:
        """Initializes the SResNet.

        Args:
            in_channels: Number of channels in the input frames.
            num_classes: Number of output classes.
            beta: Membrane-decay constant for all LIF neurons.
            stem_channels: Number of channels in the stem layer.
            stage_blocks: List containing the number of basic blocks for each stage.
            stage_channels: List containing the number of channels for each stage.
        """
        super(SResNet, self).__init__()

        self.beta = beta
        self.threshold = threshold
        self.learn_beta = learn_beta
        self.learn_threshold = learn_threshold
        self.spike_grad = spike_grad

        # Stem block
        self.stem = LayerTWrapper(
            layer=conv3x3(in_channels=in_channels, out_channels=stem_channels),
            batch_norm=nn.BatchNorm3d(num_features=stem_channels),
        )

        self.lif_stem = LIFTWrapper(layer=snn.Leaky(
            beta=beta,
            threshold=threshold,
            init_hidden=True,
            learn_beta=learn_beta,
            learn_threshold=learn_threshold,
            spike_grad=spike_grad
        ))

        # Residual blocks
        self.stages = nn.ModuleList()

        # Creates the stages that compose the residual part of the architecture
        in_c = stem_channels
        for blocks, out_c in zip(stage_blocks, stage_channels):
            # Appends a customized stage
            self.stages.append(
                self._make_stage(num_blocks=blocks, in_channels=in_c, out_channels=out_c)
            )
            # Updates the input channels for the next stage
            in_c = out_c

        # Global pool
        self.t_avg_pool = LayerTWrapper(layer=nn.AdaptiveAvgPool2d((1, 1)))

        # MLP head
        self.mlp = LayerTWrapper(layer=nn.Linear(in_features=in_c, out_features=num_classes, bias=False))

    def _make_stage(self, num_blocks: int, in_channels: int, out_channels: int) -> nn.Sequential:
        """Builds a SResNet stage with the specific configuration.

        Args:
            num_blocks: How many SResNet blocks to stack.
            in_channels: Number of input channels for the stage.
            out_channels: Number of output channels for the stage.

        Returns:
            A sequential container with the specified configuration.
        """
        layers = []

        for block_i in range(num_blocks):
            # Selects the stride based on the block index and the matching of input and output channels
            stride = 2 if (block_i == 0 and in_channels != out_channels) else 1

            # Adds a block
            layers.append(SResNetBlock(
                in_channels=in_channels,
                out_channels=out_channels,
                stride=stride,
                beta=self.beta,
                threshold=self.threshold,
                learn_beta=self.learn_beta,
                learn_threshold=self.learn_threshold,
                spike_grad=self.spike_grad
            ))

            # Updates the input channels for the next block
            in_channels = out_channels

        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass.

        Args:
            x: Input tensor of shape [T, B, C, H, W].

        Returns:
            torch.Tensor: Logits for every time step if the model is in training mode, shape [T, B, K],
                mean of logits over time steps if the model is in eval mode, shape [B, K].
        """
        # Stem block
        x = self.lif_stem(self.stem(x))

        # Residual stages
        for stage in self.stages:
            x = stage(x)

        # Global pool
        x = self.t_avg_pool(x)
        x = x.view(x.size(0), x.size(1), x.size(2))

        # MLP
        x = self.mlp(x)

        # Regulates the output based on the model current mode
        return x if self.training else x.mean(0)


@register_backbone("sresnet18-mnist")
def sresnet18_mnist(num_classes: int) -> SResNet:
    """Instantiates a SResNet18 network for Sequential SMNIST dataset.

    Args:
        num_classes: number of output classes.

    Returns:
        A SResNet object.
    """
    spike_grad = surrogate.atan()

    return SResNet(
        in_channels=1,
        num_classes=num_classes,
        beta=0.5,
        threshold=1.0,
        spike_grad=spike_grad,
        stem_channels=64,
        stage_blocks=[2, 2, 2, 2],
        stage_channels=[64, 128, 256, 512],
        learn_beta=False,
        learn_threshold=False
    )


@register_backbone("sresnet18-cifar10")
def sresnet18_cifar10(num_classes: int) -> SResNet:
    """Instantiates a SResNet18 network for Sequential SCIFAR10 dataset.

    Args:
        num_classes: number of output classes.

    Returns:
        A SResNet object.
    """
    spike_grad = surrogate.atan()

    return SResNet(
        in_channels=3,
        num_classes=num_classes,
        beta=0.5,
        threshold=1.0,
        spike_grad=spike_grad,
        stem_channels=64,
        stage_blocks=[2, 2, 2, 2],
        stage_channels=[64, 128, 256, 512],
        learn_beta=False,
        learn_threshold=False
    )


@register_backbone("sresnet19-mnist")
def sresnet19_mnist(num_classes: int) -> SResNet:
    """Instantiates a SResNet19 network for Sequential SMNIST dataset.

    Args:
        num_classes: number of output classes.

    Returns:
        A SResNet object.
    """
    spike_grad = surrogate.atan()

    return SResNet(
        in_channels=1,
        num_classes=num_classes,
        beta=0.5,
        threshold=1.0,
        spike_grad=spike_grad,
        stem_channels=128,
        stage_blocks=[3, 3, 2],
        stage_channels=[128, 256, 512],
        learn_beta=False,
        learn_threshold=False
    )


@register_backbone("sresnet19-cifar10")
def sresnet19_cifar10(num_classes: int) -> SResNet:
    """Instantiates a SResNet19 network for Sequential SCIFAR10 dataset.

    Args:
        num_classes: number of output classes.

    Returns:
        A SResNet object.
    """
    spike_grad = surrogate.atan()

    return SResNet(
        in_channels=3,
        num_classes=num_classes,
        beta=0.5,
        threshold=1.0,
        spike_grad=spike_grad,
        stem_channels=128,
        stage_blocks=[3, 3, 2],
        stage_channels=[128, 256, 512],
        learn_beta=False,
        learn_threshold=False
    )
